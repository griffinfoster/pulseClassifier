{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALFABURST Event Buffer Feature Builder\n",
    "\n",
    "The ALFABURST commensal FRB search survey searches for dedisperesed pulses above a signal to noise of 10 across of 56 MHz band. Data is processed in time windows of 2^15 * 256 microseconds (~8.4 seconds), 512 frequency channels. If a pulse is detected the entire time window is recorded to disk.\n",
    "\n",
    "The vast majority of detected pulses are false-positive events due to human-made RFI. Only a small minority of events (less than 1%) is due to astrophysical sources, primarily bright pulses from pulsars. The RFI takes on a wide range of characteristics. In the processing pipeline the brightest RFI is clipped and replaced, but low-level RFI and spectra statistics still lead to an excess of false-positives.\n",
    "\n",
    "In order to automate the processing the 150000+ recorded buffers a classifier model would be useful to ***probabilistically*** classify each event. Approximately 15000 events have been labelled into 10 different categories. We can use this *labelled* data set for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '/data2/griffin/ALFABURST/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build buffer database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseBufferPklFile = BASE_DATA_PATH + 'ALFAbuffers.pkl'\n",
    "\n",
    "# load baseBufferPkl\n",
    "df = pd.read_pickle(baseBufferPklFile)\n",
    "\n",
    "# create a predicted label column with 'unlabelled' label\n",
    "df = df.assign(predictLabel=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial buffer dataframe contains a list of all buffers with meta-data such as time, beam ID, and buffer ID. There is also global statistics for each buffer usch as number of events in the buffer and the maximum SNR event. The label column is initially empty, we need to fill it with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Beam        Buffer      MJDstart        bestDM       bestSNR  \\\n",
      "count  92453.000000  92453.000000  92453.000000  92453.000000  92453.000000   \n",
      "mean       3.773150    218.654960  57495.859286   1826.484657     13.224055   \n",
      "std        2.379155    298.492012    261.931401   2913.489521     99.728634   \n",
      "min        0.000000      1.000000  57197.378446      0.000000      6.001704   \n",
      "25%        1.000000     23.000000  57281.042274      9.000000     10.587121   \n",
      "50%        5.000000    107.000000  57350.285694     18.000000     11.497226   \n",
      "75%        6.000000    294.000000  57845.909094   2730.000000     13.102362   \n",
      "max        6.000000   2025.000000  57995.132488  10039.000000  20954.304688   \n",
      "\n",
      "          BinFactor        Events         DMmax         DMmin        DMmean  \\\n",
      "count  92453.000000  9.245300e+04  92453.000000  92453.000000  92453.000000   \n",
      "mean      14.522752  6.196611e+03   3221.769508    602.744697   1940.321632   \n",
      "std       18.036017  3.171940e+04   4171.687561   1497.618585   2638.089237   \n",
      "min        1.000000  1.000000e+00      3.000000      0.000000      3.000000   \n",
      "25%        2.000000  7.000000e+00     13.000000      5.000000      9.327869   \n",
      "50%        8.000000  6.900000e+01    347.000000      9.000000    138.544656   \n",
      "75%       16.000000  1.496000e+03   8406.000000    426.000000   4433.132580   \n",
      "max       64.000000  2.135578e+06  57287.182376  10039.000000  10039.000000   \n",
      "\n",
      "           ...            SNRmean     SNRmedian        SNRstd        MJDmax  \\\n",
      "count      ...       92453.000000  92453.000000  83811.000000  92453.000000   \n",
      "mean       ...          10.956465     10.770150      0.811790  57495.859346   \n",
      "std        ...          58.051005     49.254914     33.365527    261.931403   \n",
      "min        ...           6.001704      6.001704      0.000054  57197.378488   \n",
      "25%        ...          10.268029     10.221017      0.239563  57281.042277   \n",
      "50%        ...          10.479036     10.394923      0.417147  57350.285785   \n",
      "75%        ...          10.785488     10.617970      0.710014  57845.909171   \n",
      "max        ...       12332.098730  10400.867188   6749.039746  57995.132489   \n",
      "\n",
      "             MJDmin        MJDstd       MJDmean     MJDmedian    Label  \\\n",
      "count  92453.000000  8.381100e+04  92453.000000  92453.000000  92453.0   \n",
      "mean   57495.859313  1.090707e-05  57495.859330  57495.859330     -1.0   \n",
      "std      261.931393  1.106266e-05    261.931398    261.931398      0.0   \n",
      "min    57197.378486  0.000000e+00  57197.378488  57197.378488     -1.0   \n",
      "25%    57281.042277  4.578957e-07  57281.042277  57281.042277     -1.0   \n",
      "50%    57350.285784  7.354187e-06  57350.285785  57350.285784     -1.0   \n",
      "75%    57845.909099  2.074684e-05  57845.909117  57845.909108     -1.0   \n",
      "max    57995.132488  6.888869e-05  57995.132489  57995.132489     -1.0   \n",
      "\n",
      "       predictLabel  \n",
      "count       92453.0  \n",
      "mean           -1.0  \n",
      "std             0.0  \n",
      "min            -1.0  \n",
      "25%            -1.0  \n",
      "50%            -1.0  \n",
      "75%            -1.0  \n",
      "max            -1.0  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "['datfile' 'Beam' 'TSID' 'Buffer' 'MJDstart' 'bestDM' 'bestSNR' 'BinFactor'\n",
      " 'Events' 'DMmax' 'DMmin' 'DMmean' 'DMmedian' 'DMstd' 'SNRmean' 'SNRmedian'\n",
      " 'SNRstd' 'MJDmax' 'MJDmin' 'MJDstd' 'MJDmean' 'MJDmedian' 'Label'\n",
      " 'predictLabel']\n"
     ]
    }
   ],
   "source": [
    "print df.describe()\n",
    "print df.columns.values #each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional buffer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found features in /data2/griffin/ALFABURST/labelled/\n",
      "15070\n"
     ]
    }
   ],
   "source": [
    "# metadata and features pickles\n",
    "picklepath = BASE_DATA_PATH + 'labelled/'\n",
    "#baseDedispDirs = [BASE_DATA_PATH + 'test/']\n",
    "\n",
    "metaPklFns = glob.glob(picklepath + '*.meta.pkl')\n",
    "\n",
    "if len(metaPklFns) > 0: #if atleast one of these directories exists                             \n",
    "    print 'Found features in ' + picklepath #print to confirm existence\n",
    "    print len(metaPklFns)\n",
    "                \n",
    "    for mIdx, metaPkl in enumerate(metaPklFns): #enumerate just pairs an index starting from 0 to each metaPklFns value\n",
    "\n",
    "        # Event meta-data\n",
    "        baseMetaFn = os.path.basename(metaPkl) #returns last directory in metaPkl pathname (if path ends with '/' returns nothing)\n",
    "        bufID = int(baseMetaFn.split('.')[1].split('buffer')[-1]) #not quite sure about this line? split splits a path into (head, tail)\n",
    "        metaDict = pickle.load(open(metaPkl, 'rb')) #rb = read binary (read pickle file)\n",
    "        idx = df.loc[(df['datfile']==metaDict['dat']) & (df['Buffer']==bufID)].index\n",
    "\n",
    "        df.ix[idx, 'filterbank'] = metaDict['filterbank'] \n",
    "\n",
    "        \n",
    "        # Percent of a time series which is 0\n",
    "        df.ix[idx, 'pctZero'] = metaDict.get('pctZero', 0.) #returns number of times 'pctZero occurs in metaDict? why the zero in the arg\n",
    "        # take the 0-dm time series derivative, calculate the percent of time series with derivative=0\n",
    "        df.ix[idx, 'pctZeroDeriv'] = metaDict.get('pctZeroDeriv', 0.)\n",
    "\n",
    "        \n",
    "        # Overflow counter\n",
    "        # number of values which are above 1e20 threshold\n",
    "        ofDict = metaDict.get('overflows', {'ncount': 0, 'pct': 0.})\n",
    "        df.ix[idx, 'ofCount'] = ofDict['ncount']\n",
    "        df.ix[idx, 'ofPct'] = ofDict['pct']\n",
    "\n",
    "        \n",
    "        # Longest continuous run of a constant in the dedispersed time series\n",
    "        # tuple: (maxRun, maxVal, maxRun / float(arr.size))\n",
    "        longestRun = metaDict.get('longestRun', (0, 0., 0.)) #2nd argument in .get is what get's returned if the key doesn't exist (avoids errors)\n",
    "        df.ix[idx, 'longestRun0'] = longestRun[0]\n",
    "        df.ix[idx, 'longestRun1'] = longestRun[1]\n",
    "        df.ix[idx, 'longestRun2'] = longestRun[2]\n",
    "\n",
    "        \n",
    "        # Global statistics of the DM-0 time series\n",
    "        globalTimeStats = metaDict.get('globalTimeStats', {'std': 0., 'max': 0., 'posCount': 0, \\\n",
    "                                                           'min': 0., 'negPct': 0., 'median': 0.,\\\n",
    "                                                           'meanMedianRatio': 0., 'posPct': 0.,\\\n",
    "                                                           'negCount': 0, 'maxMinRatio': 0.,\\\n",
    "                                                           'mean': 0. }) #returns null values for all metrics if globalTimeStats doesnt exist        \n",
    "        for key in globalTimeStats:\n",
    "            df.ix[idx, 'globtsStats' + key] = globalTimeStats[key]\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Global statistics of the best DM time series\n",
    "        globalDedispTimeStats = metaDict.get('globalDedispTimeStats', {'std': 0., 'max': 0., \\\n",
    "                                                           'posCount': 0,\n",
    "                                                           'min': 0., 'negPct': 0., 'median': 0.,\\\n",
    "                                                           'meanMedianRatio': 0., 'posPct': 0.,\\\n",
    "                                                           'negCount': 0, 'maxMinRatio': 0.,\\\n",
    "                                                           'mean': 0. }) \n",
    "        for key in globalDedispTimeStats:\n",
    "            df.ix[idx, 'globalDedisptsStats' + key] = globalDedispTimeStats[key]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Statistics of 16 segments of the DM-0 time series\n",
    "        windZeros = np.zeros(16) #empty matrix\n",
    "        windTime = metaDict.get('windTimeStats',{'std':windZeros, 'max':windZeros, \\\n",
    "                                                 'min':windZeros, 'snr':windZeros, \\\n",
    "                                                 'mean':windZeros})\n",
    "        for i in range(16):\n",
    "            #key has lower case first letter, previuosly was saved into dataframe with capital first letter so this could fuck up\n",
    "            for key in windTime:\n",
    "                df.ix[idx, 'windTimeStats' + key] = windTime[key][i] \n",
    "                \n",
    "\n",
    "        # Statistics of 16 segments of the best DM time series\n",
    "        windDedispTime = metaDict.get('windDedispTimeStats',{'std':windZeros, 'max':windZeros,\\\n",
    "                                                             'min':windZeros, 'snr':windZeros,\\\n",
    "                                                             'mean':windZeros})\n",
    "        for i in range(16):\n",
    "            for key in windDedispTime:\n",
    "                df.ix[idx, 'windDedispTimeStats' + key] = windDedispTime[key][i]  #concatenates each label with its corresponding number\n",
    "\n",
    "        \n",
    "        # Statistics of the coarsely pixelized spectrogram\n",
    "        pixelZeros = np.zeros((16, 4))\n",
    "        pixels = metaDict.get('pixels',{'max':pixelZeros, 'min':pixelZeros, 'mean':pixelZeros})\n",
    "        for i in range(16):\n",
    "            for j in range(4):\n",
    "                df.ix[idx, 'pixelMax_%i_%i'%(i,j)] = pixels['max'][i][j] \n",
    "                df.ix[idx, 'pixelMin_%i_%i'%(i,j)] = pixels['min'][i][j]\n",
    "                df.ix[idx, 'pixelMean_%i_%i'%(i,j)] = pixels['mean'][i][j]\n",
    "\n",
    "\n",
    "        # Gaussian testng statistics\n",
    "        GaussianTests = metaDict.get('GaussianTests', { 'kurtosis': 0, 'skew': 0, 'dpearsonomni': 0, \n",
    "                                                     'dpearsonp': 0, 'lsD': 0, 'lsp': 0, 'ks': 0})\n",
    "        for key in GaussianTests:\n",
    "            df.ix[idx, 'GaussianTests' + key] = GaussianTests[key]\n",
    "\n",
    "        \n",
    "        #Segmented Gaussian testing statistics\n",
    "        segGaussianTests = metaDict.get('segGaussianTests', { 'lillieforsmaxp': 0, 'lillieforsmaxD': 0, \n",
    "                                                           'lfDmin': 0, 'lillieforssum': 0, 'dpearson': 0\n",
    "                                                           , 'dpearsonomnisum': 0, 'dpearsonpsum': 0})\n",
    "        for key in segGaussianTests:\n",
    "            df.ix[idx, 'segGaussianTests' + key] = segGaussianTests[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df['pixelMin_1_0'].dropna()\n",
    "#print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output of labelImg2.py\n",
    "labelPKlFiles = glob.glob(BASE_DATA_PATH + 'allLabels/*.pkl')\n",
    "\n",
    "# add assigned labels to main dataframe\n",
    "for lPkl in labelPKlFiles:\n",
    "    print 'Reading labels from', lPkl\n",
    "    labelDict = pickle.load(open(lPkl, 'rb'))\n",
    "    for key,val in labelDict.iteritems():\n",
    "        fbFN = key.split('buffer')[0] + 'fil'\n",
    "        bufID = int(key.split('.')[1].split('buffer')[-1])\n",
    "        df.loc[(df['filterbank']==fbFN) & (df['Buffer']==bufID), 'Label'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df['Label'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save combined dataframe to file\n",
    "\n",
    "This would be a good point to split into a new notebook as the previous setups have been run to combine the various labels and features into a single dataframe. We will likely not need to re-run this code often, and as it takes a few minutes to run we can just save the final dataframe to file. Then use that dataframe as the starting point for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('featureDataframeInigo.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
